---
作者: LostAbaddon
created: 2024/06/03 13:46:48
tags:
---

我们先做一些基本的假设：

首先，我们约定一个系统中的元素及其属性，都可以被有限符号来进行指称。比如两个元素可以用符号A和B来分别指称，每个元素都有诸如位置、质量等属性，这些属性也可以使用符号PA、PB、MA、MB等来指称。

理论上当然可以存在拥有不可被有限符号指称的系统，比如一个系统中包含了不可数无穷多的元素，或者其元素拥有不可数无穷多个不同的属性。

在我们实际讨论的系统中，一般上述有限表达要求都是可以满足的，但还是要强调，在理论上存在不可有限表达的系统。

由于系统中的元素及其属性都是可以有限表达的，所以一个系统在某个时刻的状态也是可以被有限表达的，因此是可以用一个有限长字符串来表达的。因此系统的运动，也即系统状态随时间的改变，可以被视为一个有限长字符串随着时间而不断改变（包括内容与长度）的过程。

接着，我们约定系统中的任意一条“规律”，都可以描述为从有限个元素所构成的初态，到有限个元素所构成的末态的一种映射。那么按照第一条约定，我们可以将“规律”描述为将有限长字符串映射到另一个有限长字符串的映射。

这条约定事实上还暗含了这么一条：模态中任意一个元素及其属性，都可以用初态中的元素及其属性来有限表达。

容易证明，这样的映射是可以用图灵机来表达的。直到我们将“有限长字符串”拓展为“可数字符串”的时候，这样的映射构成的集合才会比图灵机构成的集合更大，前者的势为阿列夫1，而后者只有阿列夫0。

因此，在上述两条看起来很合理的约定下，我们便在系统及其规律与图灵机之间建立起了一种联系。

那么，系统能“涌现”出新规律的能力，本质上就是一种图灵机中“涌现”出新的图灵机的能力。而这种能力，其实就是一台已知的图灵机在某些特定的输入下能输出一台图灵机，或者更准确地说是能输出一台在当前环境下可以被编译为图灵机的字符串。新老图灵机所能接受的语言未必是相同的，事实上一般是不同的，但很显然**新图灵机的可接受语言是可以用老图灵机的可接受语言来表达的，新图灵机的字母表也可以用老图灵机的字母表来表达**。

让我们换回到非图灵机的视角，我们会发现上述要求实际上对应下面这三条原理：

1.  底层规律本身并不排斥涌现规律的出现，且足够具有可拓展性，足以蕴含后者；
2.  满足底层规律的对象的运动可以构建涌现规律的基本单元；
3.  上述构成基本单元的对象必须要拥有在整体上形成一种恰当的、让涌现规律运行起来的构型的可能。

让我们现在换一个视角，将图灵机视为从有限长字符串到有限长字符串的映射，注意，前面我们是反过来，现在其实是说明两者是等价的。其中，如果一个输入字符串不能让图灵机停机，那么对应的输出字符串我们将其约定为空字符串。注意，这样的映射本身是存在的，但却是不可计算的。

这样，任意一台图灵机都拥有一个不可计算的属性，那就是根据输入字符串的依序递归，输出字符串会构成一个序列，这个序列中可能会存在重复元素，且很显然这个序列同样是不可计算的。

进一步，我们可以将这个序列做一个拆分，将其拆分成两个集合。一个集合是所有能被环境编译为图灵机的输出字符串构成的集合，而另一个集合是不能被环境编译为图灵机的输出字符串构成的集合。我们将后者记为 $S_0$，前者为 $T_1$。由于 $T_1$ 中的元素都可以被编译为图灵机，所以自然也就能将字符串映射到字符串，因此各自都输出字符串序列同样可以分解为相同的两个集合，这样我们可以构造这样两个序列：

$$

T_{n + 1} = \bigcup_{T \in T_n} OT(T)\\

S_{n} = \bigcup_{T \in T_n} OS(T)

$$

其中 $OT(T)$ 指的是图灵机 $T$ 的输出字符串序列中可以被环境编译为图灵机的那部分构成的集合，而 $OS(T)$ 指的是图灵机 $T$ 的输出字符串序列中不能被环境编译为图灵机的那部分构成的集合。

我们可以据此来定义一台图灵机的“涌现深度”为：如果 $T_n$ 不为空而 $T_{n + 1}$ 为空，则 $n$ 被称为 $T$ 的涌现深度，记为 $D(T)$。此外，$T_n$ 中最短的图灵机的长度也是一个非常有用的量，被记为 $E_n(T)$，称为图灵机 $T$ 的“n 阶涌现长度”。这里的长度需要注意的是，由于新老图灵机的可接受语言未必是相同的，所以这里的长度都是用最初的图灵机 $T$ 的可接受语言的字母表来衡量的字符串的长度（注意，我们前面说过，新图灵机的字母表及可接受语言都可以用老图灵机的字母表及可接受语言来表达，所以这个命题才可能为真）。

现在，让我们考虑这么一个问题：对上述给定的图灵机 $T$，随机输入长度不超过 $N$ 的字符串，那么其结果会是什么呢？尤其，如果输入是完全随机的，那么其输出中能出现新图灵机，也即涌现出新规律的情况，会是什么样的呢？

我们先用 $S_n(T, N)$ 来表达 $S_n(T)$ 中输入长度不超过 $N$ 的字符串所对应的字符串输出所构成的集合，而用 $S(T, N) = \bigcup_{n} S_n(T, N)$ 来表示所有的字符串输出构成的集合。

很显然的一点是，$S_n(T, N) < \sum_{t \in T_n(T)} c(t, N) D_t(N)$，其中 $c(t, n)$ 是图灵机 $t$ 在输入长度不超过 $N$ 时的停机概率，也即蔡廷常数。$D_t(N)$ 是输入给 $t$ 的所有字符串中，其用 $T$ 的字母来表达的长度不超过 $N$。当 $t$ 的字母都可以用 $e$ 个 $T$ 的字母来表达时则为 $D_t(N) = D^{N / e}$。

最后，用 $Q(T, N) = \frac{S(T, N)}{D^{N}}$ 表示输入长度不超过 $N$ 时图灵机 $T$ 的“输出能力”。

因此，当 $N < E_1(T)$ 时，我们有 $Q(T, N) \approx c(T, N)$。根据算法信息论我们可以知道，蔡廷常数 $c(T)$ 可以表达为 $c(T) = \sum_{t \in T_1(T)} D^{- L(t)}$ ，其中 $L(t)$ 是图灵机 $t$ 的长度。因此 $c(T, N)$ 显然会随着 $N$ 的增加而增加，所以图灵机 $T$ 的输出能力随输入长度的增加而增加。

另一方面，当 $N_1(T) \le N < N_2(T)$时则有 $Q(T, N) \approx c(T) + \sum_{t \in T_1(T)} c(t, N) D_t(N) D^{- N} > c(T)$。显然，随着 $N$ 的增大，它会超越越来越多的 $E_n(T)$，从而 $Q(T, N)$ 也会有越来越大。

由此可见，图灵机的输出能力增加来源于两个方面，一个是蔡廷常数本身随着输入长度的增加会增加，另一个则是输出长度越长，图灵机能产生的新的高阶图灵机也就越多，从而输出能力也就会越大。

用涌现的话来说，就是**随着系统越来越大，能涌现出来的新规律也会越来越多**。

其中最具有标志性的就是 $E_1(T)$，它其实就是 $T$ 能表达的图灵机中 K 氏长度最短的图灵机的 K 氏长度。在算法信息论中我们知道，当一台图灵机的 K 氏长度大于某个和语言相关的特定值时，这个长度将是不可计算的。甚至于，根据蔡廷不完全定理，当图灵机的 K 氏长度超过某个与语言相关的特定值后，我们甚至都不能证明当一台图灵机的 K 氏长度足够大时，它是否大于该值。

这也就是说，从算法信息论的角度，我们虽然可以确定随着系统的增大，能涌现出来的新规律也会越多，但当系统大到一定程度后，我们无法事先计算出当系统达到多大的时候会涌现出新的规律，甚至无法证明一项新的规律会在系统达到一定程度后涌现出来。

当我们将所考虑的系统被定位为 LLM 等 AI 系统时，我们会发现这些系统从算法信息论来看本身是具备了“参数空间越大，能涌现来的能力越多”这样的“scaling law”的。尤其当我们将系统从一个随机字符系统优化到人工神经网络这种本身就具备学习能力的系统时，能力的涌现原则上会比上述随机系统更加明显和强大。但同样的，算法信息论在这种系统中依然有效，我们依然无法在能力涌现出来之前就对其有充分的事先描述。